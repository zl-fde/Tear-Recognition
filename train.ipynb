{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e93a85f-9fff-4d24-b4ef-76ae6b508752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms, models\n",
    "\n",
    "import copy\n",
    "\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def seed_torch(seed=1234):\n",
    "\trandom.seed(seed)\n",
    "\tos.environ['PYTHONHASHSEED'] = str(seed) \n",
    "\tnp.random.seed(seed)\n",
    "\ttorch.manual_seed(seed)\n",
    "\ttorch.cuda.manual_seed(seed)\n",
    "\ttorch.cuda.manual_seed_all(seed) # if using multi-GPU.\n",
    "\ttorch.backends.cudnn.benchmark = False\n",
    "\ttorch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_torch()\n",
    "\n",
    "root_path = Path.cwd()\n",
    "\n",
    "tear_data = root_path / \"tear_data\"\n",
    "tear_data_pos = tear_data / \"pos\"\n",
    "tear_data_neg = tear_data / \"neg\"\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((800,800)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "tear_dataset = ImageFolder(tear_data,\n",
    "                      transform = data_transforms\n",
    ")\n",
    "\n",
    "tear_dataloader = torch.utils.data.DataLoader(tear_dataset, batch_size=16,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "\n",
    "dataset_sizes = len(tear_dataset)\n",
    "class_names = tear_dataset.classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def bina(img, ratio):\n",
    "    img[img < ratio] = 0\n",
    "    img[img >= ratio] = 1\n",
    "    return img\n",
    "    \n",
    "\n",
    "def bina_img(batch_img):\n",
    "    for i, img in enumerate(batch_img):\n",
    "        img[1] = bina(img[1], 0.12)\n",
    "        img[2] = bina(img[2], 0.52)\n",
    "        \n",
    "        batch_img[i] = img\n",
    "    return batch_img\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=5, model_name='resnet'):\n",
    "    since = time.time()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "            print('-' * 10)\n",
    "\n",
    "       \n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        # Iterate over data.\n",
    "        for inputs, labels in tear_dataloader:\n",
    "            \n",
    "            inputs = bina_img(inputs)\n",
    "            \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        epoch_loss = running_loss / dataset_sizes\n",
    "        epoch_acc = running_corrects.double() / dataset_sizes\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}',end='\\n\\n')\n",
    "\n",
    "        # deep copy the model\n",
    "        if epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "            \n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    name_time = time.strftime(\"%Y_%m_%d_%H_%M_%S\", time.localtime()) \n",
    "    torch.save(best_model_wts, f'./models/{model_name}_{name_time}.pt')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1808e587-03aa-4038-b7f8-4a50e6c0a472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0, 'pos': 1}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tear_dataset.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4e7f944-8e22-4fdf-b507-6aafe7252573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/120\n",
      "----------\n",
      "Loss: 0.9989 Acc: 0.5917\n",
      "\n",
      "Epoch 10/120\n",
      "----------\n",
      "Loss: 0.4865 Acc: 0.7806\n",
      "\n",
      "Epoch 20/120\n",
      "----------\n",
      "Loss: 0.5261 Acc: 0.7583\n",
      "\n",
      "Epoch 30/120\n",
      "----------\n",
      "Loss: 0.5350 Acc: 0.7444\n",
      "\n",
      "Epoch 40/120\n",
      "----------\n",
      "Loss: 0.5004 Acc: 0.7667\n",
      "\n",
      "Epoch 50/120\n",
      "----------\n",
      "Loss: 0.4168 Acc: 0.8250\n",
      "\n",
      "Epoch 60/120\n",
      "----------\n",
      "Loss: 0.4663 Acc: 0.7833\n",
      "\n",
      "Epoch 70/120\n",
      "----------\n",
      "Loss: 0.3464 Acc: 0.8583\n",
      "\n",
      "Epoch 80/120\n",
      "----------\n",
      "Loss: 0.3154 Acc: 0.8722\n",
      "\n",
      "Epoch 90/120\n",
      "----------\n",
      "Loss: 0.3169 Acc: 0.8917\n",
      "\n",
      "Epoch 100/120\n",
      "----------\n",
      "Loss: 0.2688 Acc: 0.9028\n",
      "\n",
      "Epoch 110/120\n",
      "----------\n",
      "Loss: 0.1945 Acc: 0.9222\n",
      "\n",
      "Epoch 120/120\n",
      "----------\n",
      "Loss: 0.1619 Acc: 0.9444\n",
      "\n",
      "Training complete in 8m 53s\n",
      "Best val Acc: 0.950000\n"
     ]
    }
   ],
   "source": [
    "model_ft = models.resnet18(weights=None)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "model_ft = model_ft.to(device)\n",
    "model_ft = nn.DataParallel(model_ft)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.001)\n",
    "\n",
    "# exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=6, gamma=0.1)\n",
    "exp_lr_scheduler = lr_scheduler.CosineAnnealingLR(optimizer_ft, T_max=20,eta_min=0.05)\n",
    "\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=120+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c909d6e2-24e9-460a-b9b6-ffc03e6826bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepL",
   "language": "python",
   "name": "deepl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
